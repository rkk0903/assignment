{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-011\n",
        "\n",
        "1.What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "- Logistic Regression and Linear Regression are two fundamental statistical and machine learning methods used for prediction, but they serve different purposes. While Linear Regression is primarily used for predicting continuous outcomes, Logistic Regression is used for predicting categorical outcomes. Both methods are grounded in statistical modeling, but their mathematical formulation, assumptions, and applications differ significantly.\n",
        "\n",
        " - Linear Regression – Theory\n",
        "\n",
        "- Linear Regression is a regression technique where the dependent variable (outcome) is continuous. It assumes a linear relationship between the independent variables (predictors) and the dependent variable. The general equation of linear regression is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X\n",
        "1\n",
        "\t​\n",
        "\n",
        "+β\n",
        "2\n",
        "\t​\n",
        "\n",
        "X\n",
        "2\n",
        "\t​\n",
        "\n",
        "+⋯+β\n",
        "n\n",
        "\t​\n",
        "\n",
        "X\n",
        "n\n",
        "\t​\n",
        "\n",
        "+ϵ\n",
        "\n",
        "Here,\n",
        "𝑌\n",
        "Y is the dependent variable,\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "\t​\n",
        "\n",
        ",X\n",
        "2\n",
        "\t​\n",
        "\n",
        ",…,X\n",
        "n\n",
        "\t​\n",
        "\n",
        " are independent variables,\n",
        "𝛽\n",
        "β represents coefficients, and\n",
        "𝜖\n",
        "ϵ is the error term. The model tries to find the best-fitting line by minimizing the sum of squared errors (SSE) between predicted and actual values.\n",
        "\n",
        "Linear regression is widely used in scenarios such as predicting sales, house prices, student marks, or temperature. Its output can take any real number, positive or negative, and hence it is ideal for problems requiring continuous predictions.\n",
        "\n",
        "- Logistic Regression – Theory\n",
        "\n",
        "- Logistic Regression, despite its name, is not used for regression but for classification problems. It is used when the dependent variable is categorical—most commonly binary (e.g., 0/1, Yes/No, True/False). The goal of logistic regression is not to predict a continuous value but rather to estimate the probability that an observation belongs to a particular class.\n",
        "\n",
        "- A key challenge with classification is that probabilities must lie between 0 and 1. Linear regression cannot guarantee this, as its output can extend beyond these limits. Logistic regression solves this by applying the logistic (sigmoid) function:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1∣X)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X\n",
        "1\n",
        "\t​\n",
        "\n",
        "+⋯+β\n",
        "n\n",
        "\t​\n",
        "\n",
        "X\n",
        "n\n",
        "\t​\n",
        "\n",
        ")\n",
        "1\n",
        "\t​\n",
        "\n",
        "\n",
        "- This function transforms any real-valued number into a probability between 0 and 1. Once the probability is obtained, a decision threshold (commonly 0.5) is applied: if probability ≥ 0.5, the outcome is classified as Class 1; otherwise, Class 0. Logistic Regression is also extendable to multiclass problems through Multinomial Logistic Regression.\n",
        "\n",
        "Key Differences Between Logistic and Linear Regression\n",
        "\n",
        "Nature of Output:\n",
        "\n",
        "Linear Regression predicts continuous values without bounds.\n",
        "\n",
        "Logistic Regression predicts probabilities constrained between 0 and 1.\n",
        "\n",
        "Dependent Variable:\n",
        "\n",
        "Linear Regression is suitable for continuous variables (e.g., price, weight).\n",
        "\n",
        "Logistic Regression is suitable for categorical variables (e.g., pass/fail, spam/not spam).\n",
        "\n",
        "Equation Form:\n",
        "\n",
        "Linear Regression produces a straight line equation.\n",
        "\n",
        "Logistic Regression produces an S-shaped sigmoid curve.\n",
        "\n",
        "Error Function:\n",
        "\n",
        "Linear Regression minimizes Mean Squared Error (MSE).\n",
        "\n",
        "Logistic Regression minimizes Log Loss (cross-entropy loss).\n",
        "\n",
        "Applications:\n",
        "\n",
        "Linear Regression: salary prediction, stock price forecasting, demand analysis.\n",
        "\n",
        "Logistic Regression: disease detection, fraud detection, email spam classification.\n",
        "\n",
        "\n",
        "2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "  \n",
        "\n",
        "\n",
        " - Logistic Regression is a classification technique used when the dependent variable is categorical, usually binary (0/1 or Yes/No). Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts the probability of an event occurring. To achieve this, it uses the sigmoid (logistic) function.\n",
        "\n",
        "  The sigmoid function is defined as:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "1\n",
        "\t​\n",
        "\n",
        "\n",
        "where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X\n",
        "1\n",
        "\t​\n",
        "\n",
        "+⋯+β\n",
        "n\n",
        "\t​\n",
        "\n",
        "X\n",
        "n\n",
        "\t​\n",
        "\n",
        ".\n",
        "\n",
        "This function maps any real number into the range 0 to 1, making it suitable to represent probabilities. For very large positive values of\n",
        "𝑧\n",
        "z, the output approaches 1; for large negative values, it approaches 0. When\n",
        "𝑧\n",
        "=\n",
        "0\n",
        "z=0, the output is 0.5.\n",
        "\n",
        "In logistic regression, the sigmoid ensures that outputs can be interpreted as probabilities:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        ",\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "0\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "P(Y=1∣X)=σ(z),P(Y=0∣X)=1−σ(z)\n",
        "\n",
        "- This allows decisions to be made using a threshold (commonly 0.5). For example, if probability ≥ 0.5, the prediction is Class 1; otherwise, Class 0.\n",
        "\n",
        "Thus, the sigmoid function plays a crucial role in transforming linear regression outputs into meaningful probabilities for classification.\n",
        "\n",
        "3. What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "\n",
        " - Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting and improve the generalization ability of the model. Overfitting occurs when the model learns not only the underlying patterns but also the noise in the training data, resulting in poor performance on new, unseen data.\n",
        "\n",
        " - In Logistic Regression, the model estimates coefficients (\n",
        "𝛽\n",
        "β) for each feature. If the dataset has many features, especially correlated or irrelevant ones, the coefficients may become very large in magnitude. This leads to a highly complex decision boundary that fits the training data too well but fails to generalize. Regularization solves this by adding a penalty term to the cost function to discourage very large coefficient values.\n",
        "\n",
        "The modified cost function becomes:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝛽\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝛽\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "⋅\n",
        "𝑃\n",
        "𝑒\n",
        "𝑛\n",
        "𝑎\n",
        "𝑙\n",
        "𝑡\n",
        "𝑦\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "\t​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "\t​\n",
        "\n",
        "[y\n",
        "i\n",
        "\t​\n",
        "\n",
        "log(h\n",
        "β\n",
        "\t​\n",
        "\n",
        "(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "))+(1−y\n",
        "i\n",
        "\t​\n",
        "\n",
        ")log(1−h\n",
        "β\n",
        "\t​\n",
        "\n",
        "(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "))]+λ⋅Penalty\n",
        "\n",
        "Here,\n",
        "𝜆\n",
        "λ is the regularization parameter that controls the strength of the penalty.\n",
        "\n",
        "Types of Regularization in Logistic Regression\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "\n",
        "Adds penalty = sum of absolute values of coefficients (\n",
        "∣\n",
        "𝛽\n",
        "∣\n",
        "∣β∣).\n",
        "\n",
        "Encourages sparsity by shrinking some coefficients to exactly zero → useful for feature selection.\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "\n",
        "Adds penalty = sum of squared coefficients (\n",
        "𝛽\n",
        "2\n",
        "β\n",
        "2\n",
        ").\n",
        "\n",
        "Distributes weights more evenly and avoids extremely large coefficients.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "\n",
        "Prevents overfitting by keeping model simpler.\n",
        "\n",
        "Improves generalization to unseen data.\n",
        "\n",
        "Handles multicollinearity among features.\n",
        "\n",
        "Enhances stability of coefficient estimates.\n",
        "\n",
        "\n",
        "4 .What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "\n",
        "\n",
        " - Evaluation Metrics for Classification Models\n",
        "\n",
        " - In machine learning, classification models are used to predict categorical outcomes such as spam vs. non-spam or disease vs. no disease. Evaluating these models requires more than just accuracy, as datasets are often imbalanced and misclassifications can have different consequences. Common evaluation metrics help measure performance in different aspects and ensure the model is both reliable and useful.\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "Accuracy is the proportion of correctly predicted observations out of the total observations:\n",
        "\n",
        "𝐴\n",
        "𝑐\n",
        "𝑐\n",
        "𝑢\n",
        "𝑟\n",
        "𝑎\n",
        "𝑐\n",
        "𝑦\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "\t​\n",
        "\n",
        "\n",
        "TP (True Positives): Correctly predicted positives.\n",
        "\n",
        "TN (True Negatives): Correctly predicted negatives.\n",
        "\n",
        "FP (False Positives): Incorrectly predicted positives.\n",
        "\n",
        "FN (False Negatives): Incorrectly predicted negatives.\n",
        "\n",
        "Accuracy is intuitive but may be misleading in imbalanced datasets (e.g., predicting all samples as negative in a 95:5 dataset still gives 95% accuracy).\n",
        "\n",
        "2. Precision\n",
        "\n",
        "Precision measures how many predicted positives are actually correct:\n",
        "\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "It is crucial in scenarios where false positives are costly (e.g., predicting an email as spam when it is not).\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "Recall measures how many actual positives are correctly identified:\n",
        "\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "It is important when missing positive cases is risky, such as in medical diagnoses.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "The F1-score is the harmonic mean of precision and recall:\n",
        "\n",
        "𝐹\n",
        "1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "⋅\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "+\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "F1=2⋅\n",
        "Precision+Recall\n",
        "Precision⋅Recall\n",
        "\t​\n",
        "\n",
        "\n",
        "It balances both metrics, making it useful in imbalanced datasets.\n",
        "\n",
        "5. ROC Curve and AUC\n",
        "\n",
        "The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various thresholds. The AUC (Area Under Curve) measures the model’s ability to distinguish between classes. Higher AUC indicates better performance.\n",
        "\n",
        "Importance of Metrics\n",
        "\n",
        "They provide a comprehensive evaluation beyond accuracy.\n",
        "\n",
        "They highlight trade-offs between false positives and false negatives.\n",
        "\n",
        "They help choose the right model for specific applications (e.g., medical vs. spam filtering).\n",
        "\n"
      ],
      "metadata": {
        "id": "npK9Iwf6_kXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "RL_9sLPYBbjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9357OT5gBfMK",
        "outputId": "18618c5c-3410-42a1-bc3f-e66f1932f944"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "q9jH4U9FByWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Breast Cancer)\n",
        "cancer = load_breast_cancer()\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model coefficients and intercept\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression with L2 Regularization Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya59bDf8B0gL",
        "outputId": "6e356ae9-6c06-4d25-dede-a0f04c499d9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: [0.72495553]\n",
            "Coefficients: [[ 2.08927139e+00  1.49799700e-01 -1.41631159e-01 -6.40436426e-04\n",
            "  -1.34674132e-01 -4.13270713e-01 -6.30984033e-01 -3.24834272e-01\n",
            "  -1.99045995e-01 -3.03049567e-02 -3.98195117e-02  1.48020786e+00\n",
            "  -2.84893022e-01 -7.32809892e-02 -1.44766457e-02 -1.42744032e-02\n",
            "  -5.45267683e-02 -3.60310221e-02 -4.08383335e-02  3.87857051e-03\n",
            "   1.27429906e+00 -4.05904097e-01 -4.57253207e-02 -2.68405099e-02\n",
            "  -2.47299943e-01 -1.22543502e+00 -1.59926231e+00 -5.86922223e-01\n",
            "  -7.08839532e-01 -1.19274584e-01]]\n",
            "Logistic Regression with L2 Regularization Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "3a90H6HIB-7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SlcpDAyCCN-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report (One-vs-Rest):\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j78VL4EyB_60",
        "outputId": "e6e9acab-3b3c-4378-eab1-efd6c97a4e48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (One-vs-Rest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "2jyAKWoJCSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (Breast Cancer)\n",
        "cancer = load_breast_cancer()\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=500)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],        # Regularization strength\n",
        "    'penalty': ['l1', 'l2']              # L1 (Lasso) and L2 (Ridge)\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_qQg-iWCVWs",
        "outputId": "5591a7a9-3bf2-4a3a-aa75-3614ac60a1f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9 .Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "DJoZRuHjCgL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Breast Cancer)\n",
        "cancer = load_breast_cancer()\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['target'] = cancer.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------- Without Scaling ----------------\n",
        "model_no_scaling = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ---------------- With Standardization ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling   :\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbvEDm_LCjfH",
        "outputId": "f17acf4e-9108-485c-a3d8-c53517287bc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling   : 0.9736842105263158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "\n",
        "1. Understanding the Problem\n",
        "\n",
        "  - Business Goal: Identify customers likely to respond so that marketing costs are minimized, and ROI is maximized.\n",
        "\n",
        "  - Challenge: Only 5% positives (responders) → severe class imbalance.\n",
        "\n",
        "  - If we just predict \"No response\" for everyone, we’ll get 95% accuracy but the model is useless.\n",
        "Thus, handling imbalance and proper evaluation metrics are critical.\n",
        "\n",
        " 2.Data Handling & Preprocessing\n",
        "\n",
        "  - Missing values: Handle missing data (impute or drop).\n",
        "\n",
        "  - Feature engineering: Derive features like past purchases, frequency, recency, customer demographics, browsing history.\n",
        "\n",
        "  - Categorical variables: Encode (One-Hot Encoding or Target Encoding).\n",
        "\n",
        "  - Train-test split: Stratified split to preserve class ratio.\n",
        "\n",
        "3 Feature Scaling\n",
        "\n",
        "  - Logistic Regression assumes features are on similar scales for stable optimization.\n",
        "\n",
        "   - Apply StandardScaler (z-score normalization) to numerical features.\n",
        "\n",
        "    - Helps especially when using regularization (L1/L2).\n",
        "\n",
        "- 4 .Handling Class Imbalance\n",
        "\n",
        "  - Several techniques:\n",
        "\n",
        "  - Resampling\n",
        "\n",
        "  - Oversampling minority class (e.g., SMOTE) to synthetically generate responders.\n",
        "\n",
        "  - Undersampling majority class (non-responders).\n",
        "\n",
        "  - Often use a hybrid approach.\n",
        "\n",
        "  - Class Weights\n",
        "\n",
        " - Logistic Regression has class_weight='balanced', which penalizes misclassifying the minority class more heavily.\n",
        "\n",
        "  - More robust than blind oversampling.\n",
        "\n",
        "5. Model Training & Regularization\n",
        "\n",
        "  - Use LogisticRegression(penalty='l2', solver='lbfgs', class_weight='balanced').\n",
        "\n",
        "  - Tune regularization strength (C) with GridSearchCV.\n",
        "\n",
        "  - Optionally try L1 penalty (feature selection).\n",
        "\n",
        "6. Hyperparameter Tuning\n",
        "\n",
        "  - GridSearch or RandomizedSearch over:\n",
        "\n",
        "  - C (inverse regularization strength, e.g., [0.01, 0.1, 1, 10]).\n",
        "\n",
        "  - penalty (L1, L2, ElasticNet).\n",
        "\n",
        "  - class_weight (balanced vs custom weights).\n",
        "\n",
        "  - Use Stratified K-Fold Cross Validation to maintain class balance in splits.\n",
        "\n",
        "7. Evaluation Metrics\n",
        "\n",
        "  - Since accuracy is misleading:\n",
        "\n",
        "  - Precision (how many predicted responders are correct).\n",
        "\n",
        "  - Recall (Sensitivity) (how many actual responders were captured).\n",
        "\n",
        "  - F1-score (balance between precision & recall).\n",
        "\n",
        "  - ROC-AUC (how well the model distinguishes responders from non-responders).\n",
        "\n",
        "  - PR-AUC (Precision-Recall curve) is more informative for high class imbalance.\n",
        "\n",
        "  - Confusion matrix to visualize misclassifications.\n",
        "\n",
        "8. Business Perspective\n",
        "\n",
        "  - In marketing, false positives (predicting response when no response) waste campaign costs.\n",
        "\n",
        "  - But false negatives (missing actual responders) lose potential revenue.\n",
        "\n",
        "  - Depending on business goals (minimize wasted cost vs maximize conversions), adjust decision threshold from the default 0.5.\n",
        "\n",
        "  - Example: Lowering threshold to 0.3 may catch more responders (higher recall) but increases cost.\n",
        "\n",
        "9. Deployment & Monitoring\n",
        "\n",
        "  - Deploy model and integrate with CRM for targeted campaigns.\n",
        "\n",
        "  - Continuously monitor:\n",
        "\n",
        "  - Drift in customer behavior.\n",
        "\n",
        "  - Precision/Recall over time.\n",
        "\n",
        "  - Business ROI from campaigns.\n",
        "\n",
        "  - Retrain periodically with new campaign data.\n",
        "\n",
        "✅ Final Approach (Summary)\n",
        "\n",
        "  - Clean & preprocess data (encoding, imputation).\n",
        "\n",
        "  - Standardize features.\n",
        "\n",
        "  - Handle imbalance (SMOTE + class weights).\n",
        "\n",
        "  - Train Logistic Regression with regularization.\n",
        "\n",
        "- Tune hyperparameters via stratified CV.\n",
        "\n",
        "- Evaluate using ROC-AUC, PR-AUC, F1, and threshold tuning.\n",
        "\n",
        "- Optimize threshold for business trade-off between cost & conversions.\n",
        "\n",
        "- Deploy and monitor ROI in real-world use."
      ],
      "metadata": {
        "id": "tpJY_WnWCrhv"
      }
    }
  ]
}
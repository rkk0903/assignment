{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-014\n",
        "\n",
        "\n",
        "\n",
        "1.What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "- Ensemble Learning in machine learning is a powerful technique where multiple models (called “weak learners” or “base models”) are combined to create a single stronger model (called an “ensemble”). The idea is that while an individual model may have limitations or errors, combining several models can reduce errors and improve accuracy, robustness, and generalization.\n",
        "\n",
        "-  Key Idea Behind Ensemble Learning\n",
        "\n",
        " - The key idea is based on the principle that:\n",
        "\n",
        " - “A group of weak learners, when combined appropriately, can perform better than a single strong learner.”\n",
        "\n",
        " - Each model may capture different patterns or make different types of mistakes.\n",
        "\n",
        " - By aggregating their predictions (through averaging, voting, or weighted combinations), the overall error is reduced.\n",
        "\n",
        " - This works because diversity among models reduces the risk of all of them failing on the same data points.\n",
        "\n",
        "- Why Ensemble Works\n",
        "\n",
        " - Reduces Bias – combining models (like in boosting) can make the overall model more flexible and closer to the true relationship.\n",
        "\n",
        " - Reduces Variance – averaging results of many models (like in bagging) stabilizes predictions and reduces overfitting.\n",
        "\n",
        " - Improves Generalization – ensemble models usually perform better on unseen data compared to a single learner.\n",
        "\n",
        "-  Common Ensemble Methods\n",
        "\n",
        " - Bagging (Bootstrap Aggregating):\n",
        "\n",
        " - Trains multiple models on different random samples of the training data.\n",
        "\n",
        " - Example: Random Forest (ensemble of decision trees).\n",
        "\n",
        "- Boosting:\n",
        "\n",
        " - Trains models sequentially, where each new model focuses on correcting the mistakes of the previous ones.\n",
        "\n",
        " - Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
        "\n",
        "- Stacking:\n",
        "\n",
        " - Combines predictions of multiple models using a meta-model (a model that learns how to best combine outputs).\n",
        "\n",
        "\n",
        " 2.What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "\n",
        "  Main Idea:\n",
        "\n",
        " - Train multiple models independently on different random subsets of data (using sampling with replacement).\n",
        "\n",
        " - Final prediction is made by majority voting (classification) or averaging (regression).\n",
        "\n",
        "- Key Points:\n",
        "\n",
        " - Parallel training: Models are trained independently at the same time.\n",
        "\n",
        " - Data sampling: Each model gets a random bootstrapped dataset (some observations may repeat, some may be left out).\n",
        "\n",
        " - Reduces variance: Helps prevent overfitting by stabilizing predictions.\n",
        "\n",
        " - Weak learners used: Usually decision trees.\n",
        "\n",
        " - Example: Random Forest.\n",
        "\n",
        "-  Boosting\n",
        "\n",
        "  Main Idea:\n",
        "\n",
        " - Train models sequentially, where each new model tries to correct the errors (misclassified points) of the previous models.\n",
        "\n",
        " - Final prediction is made by a weighted combination of all models.\n",
        "\n",
        "- Key Points:\n",
        "\n",
        " - Sequential training: Later models depend on the errors of earlier models.\n",
        "\n",
        " - Focus on mistakes: Misclassified data points get higher weights so the next model focuses on them.\n",
        "\n",
        " - Reduces bias and variance: Produces a strong learner from weak learners.\n",
        "\n",
        " - Weak learners used: Usually shallow decision trees (stumps).\n",
        "\n",
        " - Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
        "\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "\n",
        "- Bootstrap Sampling\n",
        "\n",
        " - Definition:\n",
        "Bootstrap sampling is a random sampling technique with replacement, used to generate multiple datasets from the original training set.\n",
        "\n",
        " - If you have a dataset with N samples, you create a new dataset by randomly  - picking N samples from it, with replacement.\n",
        "\n",
        " - “With replacement” means the same data point can appear multiple times in the sample, while some points may be left out.\n",
        "\n",
        " -  Example:\n",
        "Original dataset = {1, 2, 3, 4, 5}\n",
        "Bootstrap sample (size 5) could be = {2, 5, 2, 1, 4}\n",
        "\n",
        " - Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "- Bagging = Bootstrap Aggregating.\n",
        "The bootstrap sampling step is what creates diversity among the base learners.\n",
        "\n",
        " - Here’s how it works in methods like Random Forest:\n",
        "\n",
        "- Generate bootstrap datasets\n",
        "\n",
        " - From the original training data, create multiple random samples using bootstrap sampling.\n",
        "\n",
        " - Each base learner (e.g., a decision tree) gets a different dataset.\n",
        "\n",
        "- Train base learners independently\n",
        "\n",
        " - Since the datasets are different, each tree learns slightly different patterns.\n",
        "\n",
        " - This ensures diversity (not all trees overfit in the same way).\n",
        "\n",
        "- Aggregate predictions\n",
        "\n",
        " - In classification → take a majority vote.\n",
        "\n",
        " - In regression → take the average.\n",
        "\n",
        "- Reduce variance\n",
        "\n",
        " - Individual decision trees are high variance models (they can overfit).\n",
        "\n",
        " - By averaging multiple diverse trees trained on bootstrap samples, Bagging reduces variance and improves generalization.\n",
        "\n",
        "  Example with Random Forest\n",
        "\n",
        " - Suppose you have 1,000 training samples.\n",
        "\n",
        " - You want to build 100 trees.\n",
        "\n",
        "- For each tree:\n",
        "\n",
        " - Randomly sample 1,000 records with replacement (so some appear multiple times, others may not appear at all).\n",
        "\n",
        " - Grow a decision tree on this dataset.\n",
        "\n",
        " - Final prediction = average of all 100 trees (regression) or majority vote (classification).\n",
        "\n",
        " - This is why Random Forest is powerful:\n",
        "\n",
        " - Bootstrap sampling → brings diversity.\n",
        "\n",
        " - Aggregation → cancels out noise/errors of individual trees.\n",
        "\n",
        "\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "\n",
        "- Out-of-Bag (OOB) Samples\n",
        "\n",
        " - When we do bootstrap sampling in Bagging/Random Forest:\n",
        "\n",
        " - For a dataset with N samples, we draw N samples with replacement to create a bootstrap dataset.\n",
        "\n",
        " - On average, each bootstrap sample contains about 63% of the original data points (because of sampling with replacement).\n",
        "\n",
        " - The remaining ~37% of data points are not included in that bootstrap sample.\n",
        "\n",
        " - These left-out points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "- How OOB Samples Are Used\n",
        "\n",
        " - For each tree in the ensemble, the data points that were not used to train that tree (OOB samples) act like a validation/test set.\n",
        "\n",
        " - The trained tree can be tested on its OOB samples to check prediction accuracy.\n",
        "\n",
        " - Since each data point is likely to be OOB for several trees (not all), we can aggregate the predictions from those trees for that point.\n",
        "\n",
        "- OOB Score\n",
        "\n",
        " - The OOB score is an internal validation accuracy estimate for Bagging/Random Forest models.\n",
        "\n",
        "- How it’s computed:\n",
        "\n",
        " - For each data point in the dataset:\n",
        "\n",
        " - Collect predictions only from the trees where this point was OOB.\n",
        "\n",
        " - Aggregate those predictions (majority vote for classification, average for regression).\n",
        "\n",
        " - Compare the aggregated prediction with the actual value.\n",
        "\n",
        " - Compute accuracy (or error) across all data points.\n",
        "\n",
        " - This accuracy is called the OOB Score.\n",
        "\n",
        "- Advantages of OOB Score\n",
        "\n",
        " - No need for separate validation set → makes efficient use of the data.\n",
        "\n",
        " - Built-in unbiased error estimate → especially useful when dataset is small.\n",
        "\n",
        " - Reduces overfitting risk → gives a realistic performance measure during training.\n",
        "\n",
        "- Example (Random Forest)\n",
        "\n",
        " - Suppose you build 500 trees on a dataset of 10,000 samples.\n",
        "\n",
        " - Each tree is trained on a bootstrap dataset (≈ 6,300 samples).\n",
        "\n",
        " - The remaining ≈ 3,700 samples are OOB for that tree.\n",
        "\n",
        " - Each sample is likely to be OOB for about 1/3rd of the trees.\n",
        "\n",
        " - You use those predictions to calculate the OOB score (say, 92%).\n",
        "\n",
        " - This OOB score serves as a reliable estimate of test accuracy without needing cross-validation.\n",
        "\n",
        "5.Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "\n",
        "- Feature Importance in a Single Decision Tree\n",
        "\n",
        " - In a Decision Tree, feature importance is calculated based on how much each feature contributes to reducing impurity in the splits.\n",
        "\n",
        "- Impurity Measures (depending on task):\n",
        "\n",
        " - Classification → Gini Index or Entropy.\n",
        "\n",
        " - Regression → Variance Reduction (Mean Squared Error).\n",
        "\n",
        "- Process:\n",
        "\n",
        " - Every time the tree splits on a feature, it reduces impurity.\n",
        "\n",
        " - The reduction is attributed to that feature.\n",
        "\n",
        " - Sum up all impurity reductions for each feature across the tree.\n",
        "\n",
        " - Normalize to get a percentage score.\n",
        "\n",
        "- Limitation:\n",
        "\n",
        " - A single tree is unstable — small changes in data can drastically change which features appear at the top and how much importance they get.\n",
        "\n",
        " - Can be biased toward features with more categories (categorical) or continuous features with many split points.\n",
        "\n",
        "- Feature Importance in a Random Forest\n",
        "\n",
        " - Random Forest is an ensemble of many trees (each trained on bootstrap samples + random subset of features).\n",
        "\n",
        "- Process:\n",
        "\n",
        " - Compute feature importance individually in each tree (same way as above: impurity reduction).\n",
        "\n",
        " - Average (or sum) the importance scores across all trees.\n",
        "\n",
        " - Normalize scores so they add up to 1 (or 100%).\n",
        "\n",
        "- Advantages over single tree:\n",
        "\n",
        " - More stable (less sensitive to random fluctuations in training data).\n",
        "\n",
        " - Less biased toward features with many categories, because not all trees see the same features (due to feature subsampling).\n",
        "\n",
        " - Provides a more robust, reliable measure of which features matter most overall.\n"
      ],
      "metadata": {
        "id": "2HtbpQR_Q_q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Load the Breast Cancer dataset using\n",
        "### sklearn.datasets.load_breast_cancer()bold text\n",
        "### ● Train a Random Forest Classifier\n",
        "### ● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "b3SRU5mWXhA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create dataframe for feature importance\n",
        "feat_imp = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feat_imp = feat_imp.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print top 5\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feat_imp.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un9vFHClYA36",
        "outputId": "bc401654-d9fe-42c7-b7c5-7e4b5c5b7e54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Write a Python program to:\n",
        "### ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "### ● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "h8WLoqpwYtk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,        # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Single Decision Tree: {:.2f}\".format(acc_dt))\n",
        "print(\"Accuracy of Bagging Classifier:   {:.2f}\".format(acc_bag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_AdppdQYzkp",
        "outputId": "e1112cb8-331e-4959-b9cf-023a26c8cd79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.00\n",
            "Accuracy of Bagging Classifier:   1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Write a Python program to:\n",
        "### ● Train a Random Forest Classifier\n",
        "### ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "### ● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "joOFWCpFZHCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],   # number of trees\n",
        "    'max_depth': [2, 4, 6, None]      # depth of trees\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate final model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PaBCmVqZPh9",
        "outputId": "f9e41f0d-aac0-480e-a4e5-837d9d5776e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'n_estimators': 50}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9: Write a Python program to:\n",
        "### ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "### Housing dataset\n",
        "### ● Compare their Mean Squared Errors (MSE)bold text"
      ],
      "metadata": {
        "id": "KudLNX5OZVk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# Compute MSE\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwwifhkSZrZf",
        "outputId": "26519c54-a919-46f6-a759-bbfc83fd9a3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25592438609899626\n",
            "Mean Squared Error (Random Forest Regressor): 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10: You are working as a data scientist at a financial institution to predict loan\n",
        "### default. You have access to customer demographic and transaction history data.\n",
        "### You decide to use ensemble techniques to increase model performance.\n",
        "### Explain your step-by-step approach to:\n",
        "### ● Choose between Bagging or Boosting\n",
        "### ● Handle overfitting\n",
        "### ● Select base models\n",
        "### ● Evaluate performance using cross-validation\n",
        "### ● Justify how ensemble learning improves decision-making in this real-world\n",
        "### context.\n",
        "\n",
        "- Step-by-Step Approach for Loan Default Prediction using Ensemble Learning\n",
        " - 1. Choosing Between Bagging or Boosting\n",
        "\n",
        "- Bagging (Bootstrap Aggregating):\n",
        "\n",
        " - Works well when the base model (e.g., Decision Trees) has high variance.\n",
        "\n",
        " - It reduces variance by training multiple models on bootstrapped samples and averaging results.\n",
        "\n",
        " - Example: Random Forest.\n",
        "\n",
        "- Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM):\n",
        "\n",
        " - Sequentially builds models where each new model corrects errors of the previous one.\n",
        "\n",
        " - Reduces bias and variance, making it suitable for imbalanced datasets like loan defaults.\n",
        "\n",
        "- Choice in this case:\n",
        "\n",
        " Since loan default prediction is a highly imbalanced, complex problem, Boosting (XGBoost/LightGBM) is generally preferred because it can handle:\n",
        "\n",
        " - Non-linear relationships\n",
        "\n",
        " - Imbalance via weighted loss functions\n",
        "\n",
        " - Better performance in financial risk prediction tasks\n",
        "\n",
        " - Final Decision: Start with Boosting (XGBoost/LightGBM) but also compare with Bagging (Random Forest) as a baseline.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "- Overfitting is a major concern in financial data. Techniques include:\n",
        "\n",
        " - Regularization in Boosting models (e.g., max_depth, learning_rate, min_child_weight, reg_lambda)\n",
        "\n",
        " - Early stopping based on validation AUC/Accuracy\n",
        "\n",
        " - Cross-validation to tune hyperparameters\n",
        "\n",
        " - Feature selection using importance ranking or domain knowledge (remove redundant/irrelevant features)\n",
        "\n",
        " - Bagging methods (like Random Forest) naturally reduce overfitting by averaging across multiple trees.\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        " - Decision Trees → Most common base learners for both Bagging & Boosting.\n",
        "\n",
        " - Logistic Regression → Useful as a baseline, especially when interpretability is key.\n",
        "\n",
        " - Neural Networks → Could be used in advanced stacking ensembles but risk higher overfitting.\n",
        "\n",
        " - Final Choice:\n",
        "\n",
        "  For Bagging: Decision Trees (Random Forest)\n",
        "\n",
        "   For Boosting: Decision Trees (XGBoost/LightGBM/CatBoost)\n",
        "\n",
        "4. Evaluating Performance using Cross-Validation\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation to ensure class imbalance is respected.\n",
        "\n",
        "- Key metrics:\n",
        "\n",
        " - ROC-AUC Score → captures tradeoff between sensitivity and specificity\n",
        "\n",
        " - Precision-Recall AUC → more informative in imbalanced data (loan default prediction)\n",
        "\n",
        " - Confusion Matrix → check false positives (granting loan to defaulter is riskier than rejecting a good customer)\n",
        "\n",
        " - Perform GridSearchCV or RandomizedSearchCV for hyperparameter tuning.\n",
        "\n",
        "5. Justification: How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "- Financial institutions require high accuracy + low false positives\n",
        "\n",
        " - False Negative → Bank thinks customer will repay, but customer defaults (high financial risk)\n",
        "\n",
        " - False Positive → Bank rejects a good customer (loss of business, but safer)\n",
        "\n",
        "- Bagging helps:\n",
        "\n",
        " - Reduces variance → more stable predictions\n",
        "\n",
        "- Boosting helps:\n",
        "\n",
        " - Reduces both bias & variance\n",
        "\n",
        " - Focuses on hard-to-classify customers (edge cases like borderline defaults)\n",
        "\n",
        "- Overall benefits:\n",
        "\n",
        " - More robust and accurate credit risk assessment\n",
        "\n",
        " - Supports data-driven decision-making in lending\n",
        "\n",
        " - Reduces Non-Performing Assets (NPAs)\n",
        "\n",
        " - Increases trust in automated loan approval systems"
      ],
      "metadata": {
        "id": "TBYfLOAkaNJW"
      }
    }
  ]
}
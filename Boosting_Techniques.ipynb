{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-015\n",
        "\n",
        "\n",
        "1.What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "\n",
        "- What is Boosting?\n",
        "\n",
        "- A weak learner is typically a simple model (e.g., a shallow Decision Tree or \"stump\") that may have slightly better than random performance (say 55–60% accuracy).\n",
        "\n",
        " - Boosting works in a sequential manner:\n",
        "\n",
        " - The first weak learner is trained on the original dataset.\n",
        "\n",
        " - The next learner is trained on the same dataset but gives more weight to the misclassified samples from the previous learner.\n",
        "\n",
        " - This process repeats, and each new learner focuses on the errors of the previous ones.\n",
        "\n",
        " - Finally, all weak learners are combined (weighted majority voting or weighted sum of predictions) to form a strong model.\n",
        "\n",
        "- How Boosting Improves Weak Learners\n",
        "\n",
        " - Error-focused learning\n",
        "\n",
        " - Each new learner corrects the mistakes of the previous learners.\n",
        "\n",
        " - Example: If a Decision Stump misclassifies some points, the next stump pays more attention to those points.\n",
        "\n",
        "- Weighted combination\n",
        "\n",
        " - Each weak learner is assigned a weight based on its performance.\n",
        "\n",
        " - Better learners get higher weights, weaker ones lower weights.\n",
        "\n",
        " - The final model is a weighted sum of predictions, which reduces bias and variance.\n",
        "\n",
        "- Bias reduction\n",
        "\n",
        " - By combining many weak learners, boosting reduces bias (underfitting problem).\n",
        "\n",
        " - Even if one weak learner performs poorly, the ensemble collectively performs much better.\n",
        "\n",
        "- Adaptive improvement\n",
        "\n",
        "- - Unlike Bagging (which trains learners independently), Boosting is adaptive—later learners adapt based on earlier mistakes.\n",
        "\n",
        " - This makes it very powerful for complex data patterns.\n",
        "\n",
        "-  Example of Boosting\n",
        "\n",
        " - Suppose we want to classify emails as spam or not spam:\n",
        "\n",
        " - A single decision stump might only look at one feature (e.g., presence of the word \"offer\"). Accuracy = 55%.\n",
        "\n",
        "- Boosting trains multiple stumps:\n",
        "\n",
        " - 1st stump: focuses on \"offer\" → misclassifies some spam emails without \"offer\".\n",
        "\n",
        " - 2nd stump: focuses on \"free\" → fixes some mistakes of stump 1.\n",
        "\n",
        " - 3rd stump: focuses on \"money\" → fixes more mistakes.\n",
        "\n",
        " - Final boosted model combines them → Accuracy = 95%+.\n",
        "\n",
        "\n",
        " 2.What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        " - Idea: Train weak learners sequentially, and reweight the training samples so that misclassified samples get higher weight in the next round.\n",
        "\n",
        "- How training works:\n",
        "\n",
        " - Start with equal weights for all training samples.\n",
        "\n",
        " - Train the first weak learner (e.g., a decision stump).\n",
        "\n",
        " - Increase the weights of misclassified samples so the next learner focuses more on them.\n",
        "\n",
        " - Train the next weak learner on the reweighted dataset.\n",
        "\n",
        " - Repeat this process for several learners.\n",
        "\n",
        " - Final prediction = weighted vote/average of weak learners, where each learner’s weight depends on its accuracy.\n",
        "\n",
        " - Key Mechanism: Emphasizes difficult samples by adjusting data weights.\n",
        "\n",
        "🔹 2. Gradient Boosting\n",
        "\n",
        "- Idea: Train weak learners sequentially, but instead of reweighting samples, each new learner is trained to fit the residual errors (or gradients of the loss function) of the previous model.\n",
        "\n",
        "- How training works:\n",
        "\n",
        " - Start with an initial prediction (e.g., mean value for regression).\n",
        "\n",
        " - Compute residuals (actual – predicted) or negative gradient of loss function.\n",
        "\n",
        " - Train the next weak learner to predict these residuals.\n",
        "\n",
        " - Add the learner’s predictions (scaled by a learning rate) to improve the model.\n",
        "\n",
        " - Repeat this process, gradually reducing the errors.\n",
        "\n",
        " - Final prediction = sum of all weak learners’ contributions.\n",
        "\n",
        "\n",
        "\n",
        " 3.How does regularization help in XGBoost?\n",
        "\n",
        " - XGBoost (Extreme Gradient Boosting) is one of the most widely used machine learning algorithms because of its exceptional performance, scalability, and robustness. One of the key reasons for its success lies in its effective use of regularization techniques. Regularization refers to the process of adding penalty terms to the objective function of a model to control its complexity and prevent overfitting. In XGBoost, regularization plays a central role by ensuring that the model not only fits the training data well but also generalizes effectively to unseen data.\n",
        "\n",
        "Objective Function with Regularization\n",
        "\n",
        "The objective function in XGBoost is given as:\n",
        "\n",
        "- 𝑂\n",
        "𝑏\n",
        "𝑗\n",
        "=\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "+\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        ")\n",
        "Obj=L(\n",
        "y\n",
        "^\n",
        "\t​\n",
        "\n",
        ",y)+Ω(f)\n",
        "\n",
        "Here:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "L(\n",
        "y\n",
        "^\n",
        "\t​\n",
        "\n",
        ",y) is the loss function, which measures how well the model predictions\n",
        "𝑦\n",
        "^\n",
        "y\n",
        "^\n",
        "\t​\n",
        "\n",
        " match the actual values\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        ")\n",
        "Ω(f) is the regularization term, which penalizes the complexity of the trees.\n",
        "\n",
        "The regularization term is defined as:\n",
        "\n",
        "Ω\n",
        "(\n",
        "𝑓\n",
        ")\n",
        "=\n",
        "𝛾\n",
        "𝑇\n",
        "+\n",
        "1\n",
        "2\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑇\n",
        "𝑤\n",
        "𝑗\n",
        "2\n",
        "Ω(f)=γT+\n",
        "2\n",
        "1\n",
        "\t​\n",
        "\n",
        "λ\n",
        "j=1\n",
        "∑\n",
        "T\n",
        "\t​\n",
        "\n",
        "w\n",
        "j\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑇\n",
        "T = number of leaves in the tree,\n",
        "\n",
        "𝑤\n",
        "𝑗\n",
        "w\n",
        "j\n",
        "\t​\n",
        "\n",
        " = weight (or score) assigned to leaf\n",
        "𝑗\n",
        "j,\n",
        "\n",
        "𝜆\n",
        "λ = L2 regularization parameter on leaf weights,\n",
        "\n",
        "𝛾\n",
        "γ = penalty for each additional leaf in the tree.\n",
        "\n",
        "- This formulation ensures that trees with too many leaves or overly large leaf weights are penalized, thereby reducing the risk of overfitting.\n",
        "\n",
        "- How Regularization Helps\n",
        "\n",
        " - Controls Model Complexity\n",
        "Without regularization, gradient boosting algorithms may produce deep and overly complex trees that perfectly fit the training data but fail to generalize. The parameter\n",
        "𝛾\n",
        "γ directly controls tree complexity by adding a cost for each additional leaf. Only splits that significantly reduce the loss are allowed, which results in simpler and more interpretable models.\n",
        "\n",
        "- Prevents Overfitting through Leaf Weight Penalty\n",
        "The term\n",
        "𝜆\n",
        "∑\n",
        "𝑤\n",
        "𝑗\n",
        "2\n",
        "λ∑w\n",
        "j\n",
        "2\n",
        "\t​\n",
        "\n",
        " - applies an L2 penalty on the leaf weights. This prevents leaf scores from becoming excessively large, which could cause the model to rely too heavily on certain features or noise in the data. By shrinking leaf weights, XGBoost produces smoother and more stable predictions.\n",
        "\n",
        " - Balances Bias-Variance Tradeoff\n",
        "Boosting methods generally have low bias but can suffer from high variance, especially when many trees are added. Regularization helps reduce variance by limiting model flexibility, while still maintaining low bias through sequential learning. This balance improves generalization on unseen data.\n",
        "\n",
        " - Prunes Irrelevant Splits\n",
        "During training, XGBoost calculates the gain from splitting a node. If the gain after applying the penalty term is not positive, the split is discarded. This pruning mechanism ensures that only beneficial splits are retained, which leads to compact and efficient trees.\n",
        "\n",
        " - Additional Regularization Methods\n",
        "Beyond L1 and L2 penalties, XGBoost also uses shrinkage (learning rate) and subsampling techniques. Shrinkage scales the contribution of each tree, preventing over-reliance on any single learner. Subsampling rows and columns introduces randomness, similar to bagging, which reduces correlation among trees and further prevents overfitting.\n",
        "\n",
        "\n",
        "4.Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "\n",
        "- CatBoost is considered highly efficient for handling categorical data because it directly integrates categorical feature encoding into its algorithm, instead of requiring manual preprocessing like one-hot encoding or label encoding. Here’s a detailed explanation of why it is efficient:\n",
        "\n",
        "1. Native Handling of Categorical Features\n",
        "\n",
        " - Most machine learning algorithms (e.g., XGBoost, LightGBM) require categorical variables to be manually transformed into numerical representations. Common methods like one-hot encoding can:\n",
        "\n",
        " - Blow up feature dimensions (especially with high-cardinality features).\n",
        "\n",
        " - Lose information about category relationships.\n",
        "\n",
        " - CatBoost solves this internally by transforming categorical features into numerical values in a statistically sound way, without huge dimensional expansion.\n",
        "\n",
        "2. Target-Based Encoding with Permutation\n",
        "\n",
        " - CatBoost uses a special method called “Ordered Target Statistics” (or Ordered Target Encoding):\n",
        "\n",
        " - Instead of replacing a category with the mean target value (which causes target leakage), CatBoost builds encodings based on permutations of the dataset.\n",
        "\n",
        "  For each row, the algorithm computes statistics (like mean label) from only the preceding examples in the permutation.\n",
        "\n",
        " - This ensures that the model does not “peek” at the future data while encoding, reducing target leakage and overfitting.\n",
        "\n",
        " - This encoding is more informative than one-hot and more robust than simple label encoding.\n",
        "\n",
        "3. Efficient Handling of High-Cardinality Features\n",
        "\n",
        " - One-hot encoding with thousands of categories becomes infeasible.\n",
        "\n",
        " - CatBoost handles high-cardinality categorical features naturally, without blowing up memory or runtime.\n",
        "\n",
        " - This makes it efficient for datasets with features like user IDs, product IDs, ZIP codes, etc.\n",
        "\n",
        "4. Combination of Categorical Features\n",
        "\n",
        " - CatBoost automatically generates combinations of categorical features during training.\n",
        " - For example: if you have \"City\" and \"Product Category\", it can create an interaction feature \"City × Product Category\" on the fly.\n",
        "This expands the model’s ability to capture complex relationships, without manual feature engineering.\n",
        "\n",
        "5. Reduced Overfitting\n",
        "\n",
        " - Thanks to ordered encoding and statistical techniques, CatBoost reduces the risk of overfitting on categorical variables — which is a common problem when using naive target encoding.\n",
        "\n",
        "6. Practical Efficiency\n",
        "\n",
        " - Ease of use: You can just pass categorical column indices or names to CatBoost; no manual preprocessing needed.\n",
        "\n",
        " - Training speed: Optimized C++ implementation and symmetric trees make it faster than many alternatives, even with categorical encodings.\n",
        "\n",
        " - Better accuracy: It often outperforms LightGBM and XGBoost when categorical variables are dominant.\n",
        "\n",
        "\n",
        "5.What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "\n",
        "- Why Boosting is Preferred in Some Cases\n",
        "\n",
        " - Bagging reduces variance by averaging many independent learners (good for noisy/high-variance models like decision trees).\n",
        "\n",
        " - Boosting reduces both bias and variance by sequentially correcting the mistakes of earlier models, making it more powerful for complex, structured data.\n",
        "\n",
        " - Boosting often achieves higher accuracy than bagging when the dataset is structured, imbalanced, or requires learning from complex relationships.\n",
        "\n",
        "🔹 Real-World Applications Where Boosting Excels\n",
        "1. Finance & Banking (Credit Scoring, Fraud Detection)\n",
        "\n",
        " - Boosting models (XGBoost, LightGBM) are widely used for predicting loan defaults, credit risk, and fraud detection.\n",
        "\n",
        " - These tasks involve imbalanced datasets (few fraud cases vs. many normal transactions). Boosting handles class imbalance better by focusing on hard-to-classify cases.\n",
        "\n",
        "2. Healthcare & Medical Diagnosis\n",
        "\n",
        " - Used in predicting disease risks (e.g., diabetes, cancer detection, heart disease prognosis).\n",
        "\n",
        " - Boosting is preferred because:\n",
        "\n",
        " - Medical datasets often have complex feature interactions (age × lifestyle × genetic factors).\n",
        "\n",
        " - Boosting captures nonlinear relationships better than bagging.\n",
        "\n",
        "3. Marketing & Customer Analytics\n",
        "\n",
        " - Applications: Customer churn prediction, recommendation systems, targeted ads, click-through rate prediction.\n",
        "\n",
        " - Boosting works well because:\n",
        "\n",
        " - Data is usually tabular with many categorical features (region, product category, browsing history).\n",
        "\n",
        " - CatBoost and LightGBM are optimized for categorical variables, making boosting very effective.\n",
        "\n",
        "4. Insurance (Risk Assessment & Claim Prediction)\n",
        "\n",
        " - Boosting is used for predicting insurance claims, fraud, and premium pricing.\n",
        "\n",
        " - Insurance data is highly imbalanced and requires models that can focus on rare but important cases.\n",
        "\n",
        "5. Competitions & Research (Kaggle, Data Science Challenges)\n",
        "\n",
        " - Boosting methods (XGBoost, LightGBM, CatBoost) dominate Kaggle leaderboards because:\n",
        "\n",
        " - They achieve state-of-the-art accuracy on tabular datasets.\n",
        "\n",
        " - They balance speed, accuracy, and interpretability.\n",
        "\n",
        "6. Natural Language Processing (NLP) & Text Classification\n",
        "\n",
        " - While deep learning dominates NLP, boosting is still used for:\n",
        "\n",
        " - Sentiment analysis, spam filtering, topic classification (when dataset is small/medium).\n",
        "\n",
        " - Boosting works well with sparse, high-dimensional features (like TF-IDF vectors).\n",
        "\n",
        "7. Manufacturing & Predictive Maintenance\n",
        "\n",
        " - Boosting is applied to predict equipment failures, quality control issues, supply chain risks.\n",
        "\n",
        " - These datasets often have rare failure cases — boosting focuses on the “hard” examples."
      ],
      "metadata": {
        "id": "optrJC0Dfypv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6.● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks  \n",
        ".Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "brKU6u4Hl13K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "adaboost = AdaBoostClassifier(\n",
        "    n_estimators=100,   # number of weak learners\n",
        "    learning_rate=0.5,  # step size for updating weights\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy on Breast Cancer dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI8RQCHsl2UP",
        "outputId": "616ed6f1-c412-445c-ab1f-fc68114c195c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer dataset: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "p-aKoUEMmAP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=200,   # number of boosting stages\n",
        "    learning_rate=0.1,  # step size shrinkage\n",
        "    max_depth=3,        # depth of individual trees\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate performance (R² score)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R² score on California Housing dataset:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BnfRp1wmI7u",
        "outputId": "3f982b3e-067d-4f7b-f3d4-602bb915b286"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R² score on California Housing dataset: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "J2FgkQ-BmOxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,   # number of trees\n",
        "    max_depth=3,        # depth of trees\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train models\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"XGBoost Classifier Accuracy on Breast Cancer dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfuPt4PBmUYt",
        "outputId": "9228f852-21d5-42f0-ef39-d5bdf5eefff4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.3}\n",
            "XGBoost Classifier Accuracy on Breast Cancer dataset: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:59:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "MZoXgYVamfZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,          # suppress training logs\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = cat_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"CatBoost Classifier Accuracy on Breast Cancer dataset:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "utbolChbmgs-",
        "outputId": "4d51e85e-330a-4ffc-d0ad-5d0330109a04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier Accuracy on Breast Cancer dataset: 0.9649122807017544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15705bce",
        "outputId": "a3981b07-d2b9-4ccc-b5ac-2892ed24c1aa"
      },
      "source": [
        "%pip install catboost"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Step 1: Data Preprocessing\n",
        "1. Handle Missing Values\n",
        "\n",
        " - Numeric features: impute with median (robust to outliers).\n",
        "\n",
        " - Categorical features: impute with a special category (e.g., \"Missing\") or mode.\n",
        "\n",
        " - Some boosting algorithms (CatBoost, XGBoost) can directly handle missing values during training, reducing preprocessing burden.\n",
        "\n",
        "2. Handle Categorical Features\n",
        "\n",
        " - Option A (CatBoost): Handles categorical features natively (ordered target statistics).\n",
        "\n",
        " - Option B (XGBoost/LightGBM): Encode using target encoding or one-hot encoding (for low-cardinality features).\n",
        "\n",
        "3. Feature Scaling\n",
        "\n",
        " - Boosting algorithms (tree-based) don’t require scaling, unlike linear models.\n",
        "\n",
        " - But normalization might help if additional models are compared.\n",
        "\n",
        "4. Class Imbalance\n",
        "\n",
        " - Dataset is imbalanced (fewer defaults than non-defaults).\n",
        "\n",
        " - Solutions:\n",
        "\n",
        " - Use class weights (higher weight for default class).\n",
        "\n",
        " - Use SMOTE/oversampling cautiously.\n",
        "\n",
        " - Boosting models can incorporate imbalance parameters:\n",
        "\n",
        " - scale_pos_weight in XGBoost\n",
        "\n",
        " - class_weights in CatBoost\n",
        "\n",
        "- Step 2: Choice of Algorithm\n",
        "\n",
        " - AdaBoost → simple, but weaker on high-dimensional + categorical data.\n",
        "\n",
        " - XGBoost → great performance, efficient, widely used in finance. Handles missing values. Needs encoding for categorical features.\n",
        "\n",
        " - CatBoost → best when many categorical features exist, avoids one-hot encoding, reduces overfitting risk.\n",
        "\n",
        "  Choice:\n",
        "\n",
        " - If many categorical features → CatBoost.\n",
        "\n",
        " - If mostly numeric features → XGBoost.\n",
        "\n",
        " - For a FinTech loan dataset (usually mix of both), CatBoost is a strong candidate.\n",
        "\n",
        "- Step 3: Hyperparameter Tuning\n",
        "Key Hyperparameters:\n",
        "\n",
        " - n_estimators → number of trees (too high = overfit, too low = underfit).\n",
        "\n",
        " - learning_rate → controls step size; smaller with higher n_estimators improves generalization.\n",
        "\n",
        " - max_depth → controls complexity of each tree.\n",
        "\n",
        " - subsample → fraction of samples per tree (helps prevent overfitting).\n",
        "\n",
        " - colsample_bytree (XGBoost) → feature subsampling for regularization.\n",
        "\n",
        " - scale_pos_weight (XGBoost) or class_weights (CatBoost) → handle imbalance.\n",
        "\n",
        "- Strategy:\n",
        "\n",
        " - Start with RandomizedSearchCV for coarse search.\n",
        "\n",
        " -  Refine with GridSearchCV around the best candidates.\n",
        "\n",
        " - Use cross-validation (Stratified K-Fold) to preserve class ratio.\n",
        "\n",
        "- Step 4: Evaluation Metrics\n",
        "\n",
        " - Since the dataset is imbalanced, plain accuracy is misleading.\n",
        "\n",
        "  Preferred metrics:\n",
        "\n",
        " - ROC-AUC score → measures ability to rank defaults vs non-defaults.\n",
        "\n",
        " - Precision, Recall, F1-score → especially recall (catch as many defaults as possible).\n",
        "\n",
        " - PR-AUC (Precision-Recall AUC) → more informative when defaults are rare.\n",
        "\n",
        " - Confusion Matrix → for business interpretation (false positives vs false negatives).\n",
        "\n",
        " - Business priority:\n",
        "\n",
        " - False negatives (predicting non-default when customer defaults) are costlier → focus on recall / sensitivity.\n",
        "\n",
        "-  Step 5: Business Benefits\n",
        "\n",
        " - Risk Management: Early identification of risky borrowers → reduced loan losses.\n",
        "\n",
        " - Profit Maximization: Focus lending on low-risk customers, improving ROI.\n",
        "\n",
        " - Regulatory Compliance: Model transparency (feature importance) helps explain credit decisions to regulators.\n",
        "\n",
        " - Customer Segmentation: Insights into which features drive default risk → targeted interventions (e.g., financial counseling, adjusted interest rates).\n",
        "\n",
        " - Operational Efficiency: Automated credit scoring reduces manual review workload."
      ],
      "metadata": {
        "id": "m5eO8Yzym9f7"
      }
    }
  ]
}